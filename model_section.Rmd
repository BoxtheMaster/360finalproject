---
title: "Model_section"
author: "Danny Luo"
date: "4/29/2021"
header-includes: 
  - \def\bs{\boldsymbol}
output: pdf_document
---

### Nonparametric Learning for cluster size K

We assume the score vector for each individual in the survey comes from a mixture Gaussian distribution with unknown number of components $K$ in the mixture. The normal assumption could be largely justified by the symmetric bell shape distribution of 3 dimensions in the personality data as shown in EDA. 

To find the unknown number of components or cluster number $K$, we adopt a Dirichlet process Gaussian mixture model(DPGMM).It is a widely used clustering tool documented in literature.[@gorur_dirichlet_2010] The sampling model of DPGMM has the form below:
$$
\begin{aligned} 
  y_i &\sim N(y|\bs{\theta_i}),\\
  \bs{\theta_i} = \{\bs{\mu_i},\Sigma_i\} &\sim G,\\
  G &\sim DP(\alpha, G_0)\\
\end{aligned} 
$$
The process works by first drawing a distribution $G$ from Dirichlet Process DP with concentration paramter $\alpha$ and a base distirbution of $G_0$. $G_0$ is a joint distribution of Gaussian paramters $\bs{\mu},\Sigma$, which we assume all Gaussian mixture paramters come from. The hierachial process first draw a distribution $G$ from the DP, where $G=\sum^{\infty}_{K=0} \pi_k \delta_{\bs{\theta_k}}$. That is, we can understand G as $K \rightarrow \infty$ random discrete probability measure, where $\delta_{\bs{\theta_k}}$ is a point mass centered on $\bs{\theta_k}$.[@teh_dirichlet_2010]. A stick-breaking property construction of the DP process suggests that most probability mass is concentrated on a few values, that is, when $\bs{\theta_i}$ is being simulated from $G$, it will mostly likely take on only a few discrete values given appropriate concentration value $\alpha$ and those few values become our cluster parameters $\bs{\theta_i}$. This process is non-parametric, providing the key advantage of nonfixed cluster size. Thus the cluster size is fully learned from the data itself. We place the following priors on the paramters $\alpha$ and $G_0$:
$$
\begin{aligned} 
  \alpha &\sim Gamma(a=2,b=4)\\
  G_0(\bs{\mu},\Sigma) &\sim N(\bs{\mu}|\bs{\mu_0}=0,\Sigma_0)IW(\nu_0,\Phi_0)
\end{aligned} 
$$
We chose a Gamma prior since the postive support matches that of $\alpha$, and $Gamma(2,4)$ gives us an expected value of $alpha=0.5$. A small $\alpha$ generally yields more concentrated distribution, thus less clusters as we would expect. Literature has reported a meaingful cluster size of 4 on the same personality data we used.[@gerlach_robust_2018]. For the base distribution, since the data is scaled, we will set the prior paramter $\mu_0$ to 0, $\nu_0$ to be 1 and $\Phi_0=I$ to represent a non-informative prior belief. Semi-conjugacy and full conditionals can be established for $G_0$ since we do not assume any dependency between mean and variance. For posterior sampling of $alpha$, we adopted the MCMC sampling scheme as described by West (1992). [@west_hyperparameter_1992] This scheme is used in r-package $DirichletProcess$ [@R-dirichletprocess]. Due to the complexity of the full sampling scheme, the full details and posteriors will not be discussed here at length. MCMC sampling shcemes return the following clusters. 


### Iterative Gaussian mixture Model 

We will use the posterior clusters as our prior to the Gaussian mixture model with fixed cluster size.The motivation is that since the DPGMM process is very costly, we will then "learn" the rest of the data using the fixed number of cluster size in DPGMM. This should be appropriate since we used 10k random samples that could be representative of the whole population profiles. The fixed $K$ mixture model can be described by the following:
$$
\begin{aligned} 
  y_i|z_i=j &\sim N(\bs{\mu_j},\Sigma_j),\\
  P(z_i=j)&=p_j,\\
\end{aligned} 
$$
We assigned the following prior:
$$
\begin{aligned} 
  (\bs{\mu_j},\Sigma_j) &\sim N(\bs{\mu_{0j},\Phi_j})\times Wishart (n, V)  \forall j=1,...,K ,\\
  \bs{p}&\sim Dirichlet(\bs{\alpha}),
\end{aligned} 
$$
where for each individual $i$, $z_i$ is a latent unobserved component membership variable indicating which component in the mixture it belongs to. We plug in the values of $(\bs{\mu_j},\Sigma_j)$ using cluster paramters we derive from DPGMM. $\alpha$ vector is also derived from the weights from DPGMM. 

We inference on the posterior cluster paramters by MCMC, using the r package mixAK.[@R-mixAK] We initiate the chain using the same paramters we got from DPGMM in the last step for faster convergence.

Since the methods error out when running larger set of data, we break down the rest of our data into chunks and iteratively ran the MCMC procedure, feeding the Maximum-A-Posterior estimation of last the MCMC chain into the prior of the next one. In that way, we are iteratively "learning" the population patterns. 



