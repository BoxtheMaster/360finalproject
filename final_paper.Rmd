---
title: "Bayesian Unsupervised Clustering Method For Uncovering Latent Personality Types "
author: "Boxuan Li, Nianli Peng, Danny Luo"
date: "4/24/2021"
output:
  bookdown::pdf_document2:
    toc: false
bibliography: mybib.bib
link-citations: yes
csl: ieee.csl
header-includes: 
  - \def\bs{\boldsymbol}
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```

# Introduction

The Five Factor Model (FFM) of personality is a model for personality assessment that has been widely studied and applied in the field of Psychology. [@FFM_intro] It proposes 5 domains across which one's personality could be characterized. They are Openness to Experience, Conscientiousness, Extraversion, Agreeableness and Neuroticism (or in abbreviation, OCEAN) respectively. 

While FFM presents a viable framework to evaluate individual personality's scores on those five traits, it does not identify any personality type by itself. To fully extract the value from FFM data usually means analyzing in depths the interaction between each dimensions or moving a step further in classifying individuals into homogenous personality profiles that could be interpretable under FFM.[@merz_latent_2011] Identifying those latent personality types will be of trememdous psychometric values. It will not only reveal correlations between each dimension of personality traits, but will also present us a fuller picture of compositions of human personalities. An ideal latent personality classification would also yield a simple and univariate measure of individual personality, that could be used in causal inference and prediction widely in the field of psychology and behavioral science. 

Recent literatures have attempted with various techniques to approach this clustering tasks to identify personality types from FFM, including Latent Profile Analysis, Gaussian Mixture Model combined with Factor Analysis. [@merz_latent_2011][@gerlach_robust_2018] 

We propose an Bayesian unsupervised clustering algorithm that leverages a two-fold modeling structure:

- A non-parametric Dirichlet process Gaussian mixture model to estimate size of clusters and their respective subpopulation parameters using a small portion of data
- Feed the above result as prior into Gaussian mixture model with fixed cluster size, utilizing the rest of our data

We adopt this two phase modeling due to expensive computational cost given the gigantic dataset. The final output will yield a clustering of all individuals into different latent personalities type that is highly interpretable using FFM framework. 

# Data

This dataset contains $1,015,342$ questionnaire answers collected through an interactive online personality test by Open Psychometrics from 2016 to 2018. The personality test was constructed with the "Big-Five Factor Markers" from the International Personality Item Pool, developed by Goldberg (1992). It consists of fifty items that the respondent must rate on how true they are about him/her on a five point scale from "Very Inaccurate", "Moderately Inaccurate", "Neither Inaccurate nor Accurate", "Moderately Accurate", and "Very Accurate". Responses to this test was recorded anonymously. More information about each question is included in the appendix.

In this study we will analyze the data set and use a Bayesian unsupervised learning algorithm for clustering the participants. It looks like there are 89150 missing values. After eliminating missing values, we have $1013558$ valid observations. We see that the vast majority of the participants are from the U.S. (See Appendix \@ref(EDA country)) We might be exposed to selection bias.

Among the $50$ items in the survey, some are positive (e.g. "I am the life of the party") while some are negative (e.g. "I don't talk a lot"). For $+$ keyed items, the response "Very Inaccurate" is assigned a value of $1$, "Moderately Inaccurate" a value of $2$, "Neither Inaccurate nor Accurate" a $3$, "Moderately Accurate" a $4$, and "Very Accurate" a value of $5$. For $-$ keyed items, the response "Very Inaccurate" is assigned a value of $5$, "Moderately Inaccurate" a value of $4$, "Neither Inaccurate nor Accurate" a $3$, "Moderately Accurate" a $2$, and "Very Accurate" a value of $1$.
 
Once numbers are assigned for all of the items in the scale, we will sum all the values to obtain a total scale score for each of the five personality traits. The distribution of "Extroversion'", "Neuroticism", and "Conscientiousness" looks pretty symmetric, but that of "Agreeableness" and "Openness" looks left-skewed. Since we will be approximating the distribution of trait scores as normal distributions, we should proceed with caution when analyzing these two traits.


# Model


## Nonparametric Learning for cluster size K

We assume the score vector for each individual in the survey comes from a mixture Gaussian distribution with unknown number of components $K$ in the mixture. The normal assumption could be largely justified by the symmetric bell shape distribution of 3 dimensions in the personality data as shown in EDA. 

To find the unknown number of components or cluster number $K$, we adopt a Dirichlet process Gaussian mixture model(DPGMM).It is a widely used clustering tool documented in literature.[@gorur_dirichlet_2010] The key motivation behind our adoption of this model is that Dirichlet Process Gauassian mixture is non-parametric, in that it assumes a nonfixed number of clusters $K$. It has the following advantages: 1) it learns the number of clusters $K$ from the data. It is extremely convenient especially provided that we do not have strong belief of exact number of personality types underlying this population. 2) it eliminates the necessity of the model selection procedure if we were to use parametric models. If a parametric model is adopted, optimal number of clusters would have to be tested via different runs of model with vaired $K$ using criterion like BIC.[@gerlach_robust_2018] With DPGMM, the model itself returns the optimal "posterior" size. 

The sampling model of DPGMM has the form below:
$$
\begin{aligned} 
  y_i &\sim N(y|\bs{\theta_i}),\\
  \bs{\theta_i} = \{\bs{\mu_i},\Sigma_i\} &\sim G,\\
  G &\sim DP(\alpha, G_0)\\
\end{aligned} 
$$

To give a brief overview, the process works by first drawing a distribution $G$ from Dirichlet Process DP with concentration paramter $\alpha$ and a base distirbution of $G_0$. $G_0$ is a joint distribution of Gaussian paramters $\bs{\mu},\Sigma$, which we assume all Gaussian mixture paramters come from. The hierachial process first draw a distribution $G$ from the DP, where $G=\sum^{\infty}_{K=0} \pi_k \delta_{\bs{\theta_k}}$. That is, we can understand G as $K \rightarrow \infty$ random discrete probability measure, where $\delta_{\bs{\theta_k}}$ is a point mass centered on $\bs{\theta_k}$.[@teh_dirichlet_2010]. A stick-breaking property construction of the DP process suggests that most probability mass is concentrated on a few values, that is, when $\bs{\theta_i}$ is being simulated from $G$, it will mostly likely take on only a few discrete values given appropriate concentration value $\alpha$ and those few values become our cluster parameters $\bs{\theta_i}$. 

We place the following priors on the paramters $\alpha$ and $G_0$:
$$
\begin{aligned} 
  \alpha &\sim Gamma(a=2,b=4)\\
  G_0(\bs{\mu},\Sigma) &\sim N(\bs{\mu}|\bs{\mu_0}=0,\Sigma_0)IW(\nu_0,\Phi_0)
\end{aligned} 
$$
```{r}
prior_val <- 0.5*log(10000)
```

Our prior choices are justified as follows. We chose a Gamma prior since the postive support matches that of $\alpha$, and $Gamma(2,4)$ gives us an expected value of $alpha=0.5$. Literature has shown that the prior expeceted number of clusters can be expressed using concentration paramter $alpah$ as follows: $\alpha log(N)=0.5\,times log(10000)=4.6$.[@raykov2016simple]This matches the reporting of a meaingful cluster size of 4 on the same personality data we used in a recent study,[@gerlach_robust_2018] so it makes sense for us to set the $Gamma(2,4)$ as prior for $\alpha$. For the base distribution, since the data is scaled, we will set the prior paramter $\mu_0$ to 0, $\nu_0$ to be 1 and $\Phi_0=I$ to represent a non-informative prior belief. 

Semi-conjugacy and full conditionals can be established for $G_0$ since we do not assume any dependency between mean and variance. For posterior sampling of $alpha$, we adopted the MCMC sampling scheme as described by West (1992). [@west_hyperparameter_1992] This scheme is used in r-package $DirichletProcess$ [@R-dirichletprocess]. Due to the complexity of the full sampling scheme and the output specification of the package $DirichletProcess$, the full details and posteriors will not be discussed here at length. 

```{r}
cluster_size_list <- readRDS("cluster_size_MCMC.rds")
mean_estimator <- mean(cluster_size_list)
```

Since the process is very computationally costly, we chose to run the model with a random sample of 10,000 individuals out of over 1,000,000 data in total. The chain included 1000 iterations. We provided the summary statistics from the chain as follows. For each iteration, we estimated the number of clusters through 1) use posterior draw of $\alpha$ to perform stick breaking process in getting exact number of clusters (they can be quite large, i.e. 300-400 total clusters).(we used $PosteriorClusters$ method) [@R-dirichletprocess] 2) since we are only interested the major personality types, we choose to only retain clusters with size proportion (out of 10,000) greater than $\epsilon = 0.1$. This truncated the number of clusters to a magnitude of less than 10. We used this truncated "number of clusters" to derive a mean estimate of cluster number. (The traceplot of truncated number of clusters is in Fig.) The mean estimator is $r mean_estimator$. We rounded up to $K=5$.We picked the last iteration for our posterior estimate of cluster paramters $\bs{\theta_i}$ due to the complication of taking the average over cluster paramters of different size. 


```{r,fig.align="center",fig.cap="Traceplot for number of cluster",out.width = "70%"}
#plot MCMC 

plot(1:1000,cluster_size_list,type="l",
     ylab=" truncated cluster size",xlab="Iterature")
```


## Gaussian mixture Model 

To allow our model to capture more data in population, we will use the posterior five clusters to specify a prior to the Gaussian mixture model with fixed cluster size $K$ and run a paramteric Gaussian mixture model on 200,000 individuals. The motivation is that since this process is less costly, we can run on more data to further update the "belief" on personality types and thus achieve more model generatlization. The key assumption we implicitly made here is that DPGMM on random 10,000 individuals is representative of the whole population so that at least $K$ = 5 is fairly accurate assumption to feed into paramteric Gaussian mixture. Note that it does not have the same "accuracy" requirment on cluster parameters since Bayesian learning will keep updating them using 200,000 data while $K$ is fixed throughout this phase. This process is justified since it inherently uses the "Bayesian" philosophy of using new information to update prior belief coming from limited data.

The fixed $K$ mixture model can be described by the following:
$$
\begin{aligned} 
  y_i|z_i=j &\sim N(\bs{\mu_j},\Sigma_j),\\
  P(z_i=j)&=p_j,\\
\end{aligned} 
$$
We assigned the following prior:
$$
\begin{aligned} 
  (\bs{\mu_j},\Sigma_j) &\sim N(\bs{\mu_{0j},\Phi_j})\times Wishart (n, V)  \forall j=1,...,K ,\\
  \bs{p}&\sim Dirichlet(\bs{\alpha}),
\end{aligned} 
$$
where for each individual $i$, $z_i$ is a latent unobserved component membership variable indicating which component in the mixture it belongs to. 

We inference on the posterior cluster paramters by MCMC sampling, using the r package mixAK.[@R-mixAK], which does exert some additional prior constraints. We plug in the values of $(\bs{\mu_j},\Sigma_j)$ using cluster paramters we derive from DPGMM. However, $\alpha$ vector is required to be uniform. Thus we place a small $\bs{\alpha} = \bs{1}$, representing weak prior belief, allowing the model to learn the posterior weighted towards data itself. mixAK also allows a uniform parameter $V$ and places a hyper Gamma priors on $V$. (See our elicitation of hyper prior in Appendix) Our MCMC estimator is obtained by simply taking the cluster parameters' mean across all iterations.  
```{r}
post_data$poster.mean.mu
```

# Results
```{r mixAK-clusters, fig.cap="Mean vectors of 5 identified clusters of personality types",fig.align="center"}
#read result of MCMC
post_data <- readRDS("Posterior_Final.rds")
  
#order by percentage populatoin
order_indx <- c(3,4,5,1,2)

cols <- c("red","blue","salmon3","orange","purple")

#texts map to graph in order: 4,5,1,2,3
texts <- c("overcontrolled","undercontrolled","average","role model","d")

# Plot cluster means 
par(mfrow=c(2,3))
for(i in order_indx){
  plot(x=1:5,y=post_data$poster.mean.mu[i,],ylim=c(-2,2),
       xlab = "subtraits",ylab="z-score", 
       main=,
       sub=paste(round(post_data$poster.mean.w[i]*100,2),"% of total population"),
       col=cols[i],
       bg=cols[i],
       pch=22,
       xaxt="n")
  text(x=3,y=1.5,label=texts[i],col=cols[i],pos=3,offset = 0.5,cex=1.3)
  abline(h=0, col="black",lwd=1, lty=2)
  axis(side = 1, at = 1:5,labels = c("E","N","A","C","O"))
}

#'Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness
#1: "average I type"
#2: "role model", N is below, other above
#6: "undercontrolled", all score below average, least agreeable
#7: "Overcontrolled", high N, low E.
#8: "average II type"  all within one standard deviation
```


### f

Our analysis yields five clusters each with distinct and interpretable personality types, and we plotted the mean vector of the Gaussian distribution of each type. (See Figure \@ref(fig:mixAK-clusters) The first and most heavily weighted (with a weight of 44.49%) cluster has the personality score very close to average in all subtraits category, thus representing an average type of personality. The rest of the clusters follows more interesting patterns. We denote the personality type represented by the yellow color as "role model" since it has the highest score of "Agreeableness" and "Openness" and above average in the rest of three subtraits, all socially desirable traits except for Neuroticism (it is only slightly above average so it still supports our classification nicely). we denote the personality type reprsented by the purple cluster to be "reserved role model" in that it 
dafdfad
dfd

adfadf
# Conclusion





$$f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$
\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

: Discussion & justification of the proposed Bayesian model framework
(prior and sampling model). This discussion should elicit prior information on the
problem, the data sources available, and relevant project goals. Any “downstream”
uses of the model (e.g., for prediction, optimization, ranking) should be discussed
in detail here. See project rubric for details.


This is test for referencing equation. See equation \@ref(eq:binom)

# References {-}

<div id="refs"></div>

# Appendix {-}

## EDA country 
```{r}
library(dplyr)
library(dirichletprocess)
library(ggplot2)
library(patchwork)
library(gridExtra)
```

```{r}
data_raw = read.csv("../data-final.csv", sep='\t', na.strings = "NULL")
```

```{r}
data = data.frame(data_raw)

data <- data[ -c(51:107) ]
data <- data[ -c(52:53) ]

#print(nrow(data))
#head(data)
```

```{r}
data <- na.omit(data)
#nrow(data)
```

```{r}
countries <- data %>% count(country)
countries <- countries[countries$n>=5000,]
```

```{r fig.cap="Number of participants in countries", fig.align='center',out.width="70%"}
ggplot(countries, aes(reorder(country, -n, sum), n, fill = country)) +
  geom_bar(stat="identity", width = 0.8)+
  geom_text(aes(label=n), vjust=-0.3, size=2)+
  theme_minimal()+
  theme(legend.position="none")+
  labs(title= "Countries With More Than 5000 Participants",
                      y="Participants", x = element_blank())
```

## EDA Normalcy

```{r}
pos_keyed_vars <-  c('EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',
                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', 
                    'EST8', 'EST9', 'EST10',
                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',
                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', 
                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 
                    'OPN10')
neg_keyed_vars <-  c('EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',
                    'EST2', 'EST4',
                    'AGR1', 'AGR3', 'AGR5', 'AGR7', 
                    'CSN2', 'CSN4', 'CSN6', 'CSN8', 
                    'OPN2', 'OPN4', 'OPN6')
```

```{r}
for(key in neg_keyed_vars){
  data[key]=6-data[key]
}
```

```{r}
data <- data %>% mutate(
  EXT=rowSums(data[1:10]),
  EST=rowSums(data[11:20]),
  AGR=rowSums(data[21:30]),
  CSN=rowSums(data[31:40]),
  OPN=rowSums(data[41:50]))
score_data_final <- data[,52:56]
```

```{r, fig.cap="Normalcy check",fig.align="center",out.width="70%"}
traits = c('EXT', 'EST', 'AGR', 'CSN', 'OPN')
trait_labels = c('Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness')
myplots <- list()


p1 <- ggplot(score_data_final, aes(x= score_data_final[,1] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[1])
p2 <- ggplot(score_data_final, aes(x= score_data_final[,2] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[2])
p3 <- ggplot(score_data_final, aes(x= score_data_final[,3] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[3])
p4 <- ggplot(score_data_final, aes(x= score_data_final[,4] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[4])
p5 <- ggplot(score_data_final, aes(x= score_data_final[,5] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[5])

grid.arrange(p1,p2,p3,p4,p5, nrow = 2)
```


## Hyper Prior elicitation for V for package mixAK

MixAK only allows a uniform parameter $V$, and gives the option to specify $V$ only through specifying the hyperprior parameters of a Gamma distribution.[@R-mixAK] According to the documentation page of the package, matrix $V$ is assumed to be diagonal with $\gamma_1,\gamma_2,...,\gamma_p$  on the diagonal, and for each $\gamma_j,$ $\gamma_j^{-1} \sim Gamma(g_j,h_j)$.

We checked that the covariance matrix of some ramdom subset of the population are approximatley diagonal. Since matrix $V$ is the scale matrix oarameters of the Wishart prior placed on cluster covariance matrix $\Sigma_j$ and that it does not change with different groups, we performed the following elicitation:.

- Draw 1000 random samples of size 1000 from the dataset and calulated the 5 by 5 covariance matrix $C_i$ for $i =1,2,...1000$.
- For j =1,2,..,5, extract samples $L_j=\{C_i[j,j]\}^{1000}_{i=1}$.
- Forj =1,2,..,5, fit a gamma distribution using $L_j$, using the r package $fitdistrplus$. [@R-fitdistrplus] [@R-fitdistrplus-article] and using the estimation from the output to be our $\{ g_j,h_j \}$

The fitting was highly accurate as we checked the Q-Q plot to be almost matching.


## MCMC Diagnostics for cluster means and weights

```{r}
#read result 
post_data <- readRDS("Posterior_Final.rds")
```

```{r fig.height=10,fig.width=6,out.width = "70%",fig.align="center" ,fig.cap="Traceplots and Autocorrelation plots for cluster means"}
##MCMC Diagnostics for means
N=5000


col_names=colnames(post_data$mu)


par(mfrow=c(5,2),mar=c(5,5,5,5))

for(i in 1:length(col_names)){
  plot(1:N,post_data$mu[,i],type="l",ylab=col_names[i],xlab="Iter")
  acf(post_data$mu[,i],ylab=col_names[i],main="")
}

```


```{r fig.height=10,fig.width=6,out.width = "70%",fig.cap="Traceplots and Autocorrelation plots for weights",fig.align="center"}
##MCMC Diagnostics for weights
library(coda)
N=5000


col_names=colnames(post_data$w)

par(mfrow=c(5,2),mar=c(5,5,5,5))

for(i in 1:length(col_names)){
  plot(1:N,post_data$w[,i],type="l",ylab=col_names[i],xlab="Iter")
  acf(post_data$w[,i],ylab=col_names[i],main="")
}
```


## Personality Test Questions / Comprehensive Data description.

The following items were presented on one page and each was rated on a five point scale using radio buttons. The order on page was was EXT1, AGR1, CSN1, EST1, OPN1, EXT2, etc.

- EXT1	I am the life of the party.
- EXT2	I don't talk a lot.
- EXT3	I feel comfortable around people.
- EXT4	I keep in the background.
- EXT5	I start conversations.
- EXT6	I have little to say.
- EXT7	I talk to a lot of different people at parties.
- EXT8	I don't like to draw attention to myself.
- EXT9	I don't mind being the center of attention.
- EXT10	I am quiet around strangers.
- EST1	I get stressed out easily.
- EST2	I am relaxed most of the time.
- EST3	I worry about things.
- EST4	I seldom feel blue.
- EST5	I am easily disturbed.
- EST6	I get upset easily.
- EST7	I change my mood a lot.
- EST8	I have frequent mood swings.
- EST9	I get irritated easily.
- EST10	I often feel blue.
- AGR1	I feel little concern for others.
- AGR2	I am interested in people.
- AGR3	I insult people.
- AGR4	I sympathize with others' feelings.
- AGR5	I am not interested in other people's problems.
- AGR6	I have a soft heart.
- AGR7	I am not really interested in others.
- AGR8	I take time out for others.
- AGR9	I feel others' emotions.
- AGR10	I make people feel at ease.
- CSN1	I am always prepared.
- CSN2	I leave my belongings around.
- CSN3	I pay attention to details.
- CSN4	I make a mess of things.
- CSN5	I get chores done right away.
- CSN6	I often forget to put things back in their proper place.
- CSN7	I like order.
- CSN8	I shirk my duties.
- CSN9	I follow a schedule.
- CSN10	I am exacting in my work.
- OPN1	I have a rich vocabulary.
- OPN2	I have difficulty understanding abstract ideas.
- OPN3	I have a vivid imagination.
- OPN4	I am not interested in abstract ideas.
- OPN5	I have excellent ideas.
- OPN6	I do not have a good imagination.
- OPN7	I am quick to understand things.
- OPN8	I use difficult words.
- OPN9	I spend time reflecting on things.
- OPN10	I am full of ideas.




