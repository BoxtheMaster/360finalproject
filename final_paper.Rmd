---
title: "Bayesian Unsupervised Clustering Method For Uncovering Latent Personality Types "
author: "Boxuan Li, Nianli Peng, Danny Luo"
date: "4/24/2021"
output:
  bookdown::pdf_document2
bibliography: mybib.bib
link-citations: yes
csl: ieee.csl
header-includes: 
  - \def\bs{\boldsymbol}
nocite: '@*'
---

# Introduction

The Five Factor Model (FFM) of personality is a model for personality assessment that has been widely studied and applied in the field of Psychology. [@FFM_intro] It proposes 5 domains across which one's personality could be characterized. They are Openness to Experience, Conscientiousness, Extraversion, Agreeableness and Neuroticism (or in abbreviation, OCEAN) respectively. 

While FFM presents a viable framework to evaluate individual personality's scores on those five traits, it does not identify any personality type by itself. To fully extract the value from FFM data usually means analyzing in depths the interaction between each dimensions or moving a step further in classifying individuals into homogenous personality profiles that could be interpretable under FFM.[@merz_latent_2011] Identifying those latent personality types will be of trememdous psychometric values. It will not only reveal correlations between each dimension of personality traits, but will also present us a fuller picture of compositions of human personalities. An ideal latent personality classification would also yield a simple and univariate measure of individual personality, that could be used in causal inference and prediction widely in the field of psychology and behavioral science. 

Recent literatures have attempted with various techniques to approach this clustering tasks to identify personality types from FFM, including Latent Profile Analysis, Gaussian Mixture Model combined with Factor Analysis. [@merz_latent_2011][@gerlach_robust_2018] 

We propose an Bayesian unsupervised clustering algorithm that leverages a two-fold modeling structure:

- A non-parametric Dirichlet process Gaussian mixture model to estimate size of clusters and their respective subpopulation parameters using a small portion of data
- Feed the above result as prior into Gaussian mixture model with fixed cluster size, utilizing the rest of our data

We adopt this two phase modeling due to expensive computational cost given the gigantic dataset. The final output will yield a clustering of all individuals into different latent personalities type that is highly interpretable using FFM framework. 

# Data

This dataset contains $1,015,342$ questionnaire answers collected through an interactive online personality test by Open Psychometrics from 2016 to 2018. The personality test was constructed with the "Big-Five Factor Markers" from the International Personality Item Pool, developed by Goldberg (1992). It consists of fifty items that the respondent must rate on how true they are about him/her on a five point scale from "Very Inaccurate", "Moderately Inaccurate", "Neither Inaccurate nor Accurate", "Moderately Accurate", and "Very Accurate". Responses to this test was recorded anonymously. More information about each question is included in the appendix.

In this study we will analyze the data set and use a Bayesian unsupervised learning algorithm for clustering the participants.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```

```{r}
library(dplyr)
library(dirichletprocess)
library(ggplot2)
library(patchwork)
library(gridExtra)
```

```{r}
data_raw = read.csv("../data-final.csv", sep='\t', na.strings = "NULL")
```

```{r}
data = data.frame(data_raw)

data <- data[ -c(51:107) ]
data <- data[ -c(52:53) ]

#print(nrow(data))
#head(data)
```

```{r}
#table(is.na(data))
```
It looks like there are 89150 missing values.

```{r}
data <- na.omit(data)
#nrow(data)
```
After eliminating missing values, we have $1013558$ valid observations.

```{r}
countries <- data %>% count(country)
countries <- countries[countries$n>=5000,]
```

```{r fig.cap="Number of participants in countries"}
ggplot(countries, aes(reorder(country, -n, sum), n, fill = country)) +
  geom_bar(stat="identity", width = 0.8)+
  geom_text(aes(label=n), vjust=-0.3, size=2)+
  theme_minimal()+
  theme(legend.position="none")+
  labs(title= "Countries With More Than 5000 Participants",
                      y="Participants", x = element_blank())
```
We see that the vast majority of the participants are from the U.S. We might be exposed to selection bias.

Among the $50$ items in the survey, some are positive (e.g. "I am the life of the party") while some are negative (e.g. "I don't talk a lot"). 

For $+$ keyed items, the response "Very Inaccurate" is assigned a value of $1$, "Moderately Inaccurate" a value of $2$, "Neither Inaccurate nor Accurate" a $3$, "Moderately Accurate" a $4$, and "Very Accurate" a value of $5$.
 
For $-$ keyed items, the response "Very Inaccurate" is assigned a value of $5$, "Moderately Inaccurate" a value of $4$, "Neither Inaccurate nor Accurate" a $3$, "Moderately Accurate" a $2$, and "Very Accurate" a value of $1$.
 
Once numbers are assigned for all of the items in the scale, we will sum all the values to obtain a total scale score for each of the five personality traits.

```{r}
pos_keyed_vars <-  c('EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',
                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', 
                    'EST8', 'EST9', 'EST10',
                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',
                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', 
                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 
                    'OPN10')
neg_keyed_vars <-  c('EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',
                    'EST2', 'EST4',
                    'AGR1', 'AGR3', 'AGR5', 'AGR7', 
                    'CSN2', 'CSN4', 'CSN6', 'CSN8', 
                    'OPN2', 'OPN4', 'OPN6')
```

```{r}
for(key in neg_keyed_vars){
  data[key]=6-data[key]
}
```

```{r}
data <- data %>% mutate(
  EXT=rowSums(data[1:10]),
  EST=rowSums(data[11:20]),
  AGR=rowSums(data[21:30]),
  CSN=rowSums(data[31:40]),
  OPN=rowSums(data[41:50]))
score_data_final <- data[,52:56]
```

```{r, fig.cap="Normalcy check"}
traits = c('EXT', 'EST', 'AGR', 'CSN', 'OPN')
trait_labels = c('Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness')
myplots <- list()


p1 <- ggplot(score_data_final, aes(x= score_data_final[,1] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[1])
p2 <- ggplot(score_data_final, aes(x= score_data_final[,2] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[2])
p3 <- ggplot(score_data_final, aes(x= score_data_final[,3] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[3])
p4 <- ggplot(score_data_final, aes(x= score_data_final[,4] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[4])
p5 <- ggplot(score_data_final, aes(x= score_data_final[,5] ))+
  geom_histogram(colour="black", fill="lightblue",binwidth=3)+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank())+
  labs(title= trait_labels[5])

grid.arrange(p1,p2,p3,p4,p5, nrow = 2)
```

The distribution of "Extroversion'", "Neuroticism", and "Conscientiousness" looks pretty symmetric, but that of "Agreeableness" and "Openness" looks left-skewed. Since we will be approximating the distribution of trait scores as normal distributions, we should proceed with caution when analyzing these two traits.


# Model

## Nonparametric Learning for cluster size K

We assume the score vector for each individual in the survey comes from a mixture Gaussian distribution with unknown number of components $K$ in the mixture. The normal assumption could be largely justified by the symmetric bell shape distribution of 3 dimensions in the personality data as shown in EDA. 

To find the unknown number of components or cluster number $K$, we adopt a Dirichlet process Gaussian mixture model(DPGMM).It is a widely used clustering tool documented in literature.[@gorur_dirichlet_2010] The sampling model of DPGMM has the form below:
$$
\begin{aligned} 
  y_i &\sim N(y|\bs{\theta_i}),\\
  \bs{\theta_i} = \{\bs{\mu_i},\Sigma_i\} &\sim G,\\
  G &\sim DP(\alpha, G_0)\\
\end{aligned} 
$$
The process works by first drawing a distribution $G$ from Dirichlet Process DP with concentration paramter $\alpha$ and a base distirbution of $G_0$. $G_0$ is a joint distribution of Gaussian paramters $\bs{\mu},\Sigma$, which we assume all Gaussian mixture paramters come from. The hierachial process first draw a distribution $G$ from the DP, where $G=\sum^{\infty}_{K=0} \pi_k \delta_{\bs{\theta_k}}$. That is, we can understand G as $K \rightarrow \infty$ random discrete probability measure, where $\delta_{\bs{\theta_k}}$ is a point mass centered on $\bs{\theta_k}$.[@teh_dirichlet_2010]. A stick-breaking property construction of the DP process suggests that most probability mass is concentrated on a few values, that is, when $\bs{\theta_i}$ is being simulated from $G$, it will mostly likely take on only a few discrete values given appropriate concentration value $\alpha$ and those few values become our cluster parameters $\bs{\theta_i}$. This process is non-parametric, providing the key advantage of nonfixed cluster size. Thus the cluster size is fully learned from the data itself. We place the following priors on the paramters $\alpha$ and $G_0$:
$$
\begin{aligned} 
  \alpha &\sim Gamma(a=2,b=4)\\
  G_0(\bs{\mu},\Sigma) &\sim N(\bs{\mu}|\bs{\mu_0}=0,\Sigma_0)IW(\nu_0,\Phi_0)
\end{aligned} 
$$
We chose a Gamma prior since the postive support matches that of $\alpha$, and $Gamma(2,4)$ gives us an expected value of $alpha=0.5$. A small $\alpha$ generally yields more concentrated distribution, thus less clusters as we would expect. Literature has reported a meaingful cluster size of 4 on the same personality data we used.[@gerlach_robust_2018]. For the base distribution, since the data is scaled, we will set the prior paramter $\mu_0$ to 0, $\nu_0$ to be 1 and $\Phi_0=I$ to represent a non-informative prior belief. Semi-conjugacy and full conditionals can be established for $G_0$ since we do not assume any dependency between mean and variance. For posterior sampling of $alpha$, we adopted the MCMC sampling scheme as described by West (1992). [@west_hyperparameter_1992] This scheme is used in r-package $DirichletProcess$ [@R-dirichletprocess]. Due to the complexity of the full sampling scheme, the full details and posteriors will not be discussed here at length. MCMC sampling shcemes return the following clusters. 


## Iterative Gaussian mixture Model 

We will use the posterior clusters as our prior to the Gaussian mixture model with fixed cluster size.The motivation is that since the DPGMM process is very costly, we will then "learn" the rest of the data using the fixed number of cluster size in DPGMM. This should be appropriate since we used 10k random samples that could be representative of the whole population profiles. The fixed $K$ mixture model can be described by the following:
$$
\begin{aligned} 
  y_i|z_i=j &\sim N(\bs{\mu_j},\Sigma_j),\\
  P(z_i=j)&=p_j,\\
\end{aligned} 
$$
We assigned the following prior:
$$
\begin{aligned} 
  (\bs{\mu_j},\Sigma_j) &\sim N(\bs{\mu_{0j},\Phi_j})\times Wishart (n, V)  \forall j=1,...,K ,\\
  \bs{p}&\sim Dirichlet(\bs{\alpha}),
\end{aligned} 
$$
where for each individual $i$, $z_i$ is a latent unobserved component membership variable indicating which component in the mixture it belongs to. We plug in the values of $(\bs{\mu_j},\Sigma_j)$ using cluster paramters we derive from DPGMM. $\alpha$ vector is also derived from the weights from DPGMM. 

We inference on the posterior cluster paramters by MCMC, using the r package mixAK.[@R-mixAK] We initiate the chain using the same paramters we got from DPGMM in the last step for faster convergence.

Since the methods error out when running larger set of data, we break down the rest of our data into chunks and iteratively ran the MCMC procedure, feeding the Maximum-A-Posterior estimation of last the MCMC chain into the prior of the next one. In that way, we are iteratively "learning" the population patterns. 




# Results
Posterior analyses from the fitted Bayesian model, and a translation of
such findings into meaningful & understandable conclusions for the target audience
(e.g., engineers, business managers, policy-makers, etc). See project rubric for
details.


# Conclusion





A summary of key findings and potential impacts of your project.

We assume the score vector for each individual in the survey comes from a mixture Gaussian distribution with unknown number of components $K$ in the mixture. The normal assumption could be largely justified by the symmetric bell shape distribution of 3 dimensions in the personality data as shown in EDA. 

To find the unknown number of components or cluster number $K$, we adopt a Dirichlet process Gaussian mixture model(DPGMM).It is a widely used clustering tool documented in literature.[@gorur_dirichlet_2010] It has the form below:
$$
\begin{aligned} 
  y_i &\sim N(y|\mathbf{\theta_i})\\
  \theta_i &\sim N()
\end{aligned} 
$$




















$$f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$
\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

: Discussion & justification of the proposed Bayesian model framework
(prior and sampling model). This discussion should elicit prior information on the
problem, the data sources available, and relevant project goals. Any “downstream”
uses of the model (e.g., for prediction, optimization, ranking) should be discussed
in detail here. See project rubric for details.


This is test for referencing equation. See equation \@ref(eq:binom)

# References {-}

<div id="refs"></div>

# Appendix {-}

## 1 Personality Test Questions / Comprehensive Data description.

The following items were presented on one page and each was rated on a five point scale using radio buttons. The order on page was was EXT1, AGR1, CSN1, EST1, OPN1, EXT2, etc.

- EXT1	I am the life of the party.
- EXT2	I don't talk a lot.
- EXT3	I feel comfortable around people.
- EXT4	I keep in the background.
- EXT5	I start conversations.
- EXT6	I have little to say.
- EXT7	I talk to a lot of different people at parties.
- EXT8	I don't like to draw attention to myself.
- EXT9	I don't mind being the center of attention.
- EXT10	I am quiet around strangers.
- EST1	I get stressed out easily.
- EST2	I am relaxed most of the time.
- EST3	I worry about things.
- EST4	I seldom feel blue.
- EST5	I am easily disturbed.
- EST6	I get upset easily.
- EST7	I change my mood a lot.
- EST8	I have frequent mood swings.
- EST9	I get irritated easily.
- EST10	I often feel blue.
- AGR1	I feel little concern for others.
- AGR2	I am interested in people.
- AGR3	I insult people.
- AGR4	I sympathize with others' feelings.
- AGR5	I am not interested in other people's problems.
- AGR6	I have a soft heart.
- AGR7	I am not really interested in others.
- AGR8	I take time out for others.
- AGR9	I feel others' emotions.
- AGR10	I make people feel at ease.
- CSN1	I am always prepared.
- CSN2	I leave my belongings around.
- CSN3	I pay attention to details.
- CSN4	I make a mess of things.
- CSN5	I get chores done right away.
- CSN6	I often forget to put things back in their proper place.
- CSN7	I like order.
- CSN8	I shirk my duties.
- CSN9	I follow a schedule.
- CSN10	I am exacting in my work.
- OPN1	I have a rich vocabulary.
- OPN2	I have difficulty understanding abstract ideas.
- OPN3	I have a vivid imagination.
- OPN4	I am not interested in abstract ideas.
- OPN5	I have excellent ideas.
- OPN6	I do not have a good imagination.
- OPN7	I am quick to understand things.
- OPN8	I use difficult words.
- OPN9	I spend time reflecting on things.
- OPN10	I am full of ideas.

The time spent on each question is also recorded in milliseconds. These are the variables ending in $\_E$. This was calculated by taking the time when the button for the question was clicked minus the time of the most recent other button click.

- dateload    The timestamp when the survey was started.
- screenw     The width the of user's screen in pixels
- screenh     The height of the user's screen in pixels
- introelapse The time in seconds spent on the landing / intro page
- testelapse  The time in seconds spent on the page with the survey questions
- endelapse   The time in seconds spent on the finalization page (where the user was asked to indicate if they has answered accurately and their answers could be stored and used for research. Again: this dataset only includes users who answered "Yes" to this question, users were free to answer no and could still view their results either way)
- IPC         The number of records from the user's IP address in the dataset. For max cleanliness, only use records where this value is 1. High values can be because of shared networks (e.g. entire universities) or multiple submissions
- country     The country, determined by technical information (NOT ASKED AS A QUESTION)
- lat_appx_lots_of_err    approximate latitude of user. determined by technical information, THIS IS NOT VERY ACCURATE. 
- long_appx_lots_of_err   approximate longitude of user

