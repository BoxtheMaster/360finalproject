---
title: "EDA"
author: "Danny Luo"
date: "4/18/2021"
output: pdf_document
---
```{r}
library(dplyr)
library(dirichletprocess)
```



```{r}
###model proposal

# x_i | z_i=j ~  multivariate gaussian mixture (mu_j,sigma_j)   i=1,..,n
# mu_j,sigma_j ~ Normal-InvGamma for j = 1,...,k    
# z_i ~ categorical (k category, prob vector p)  for i=1,..,n
# p ~ Dirichlet(\alpha)  

#Interested in:  joint_posterior (p,z, {mu_j,sig_j}^k )  using Gibbs Sampler

#packages 



#x_i would be of dimension 5 (originally 50)
#Gaussian assumption?
#more sophisticatd models different Bayesian clustering
#choices of distance functions. sum of square



#z vector of size 1 million
#At each iteration, we need to sample z_i for i=1,...,n
#thin data due to computation cost?


#choose value k? 


#k-mean as prior?




```

```{r}

data <- read.delim("data-final.csv",header=TRUE,skipNul= TRUE,na.strings = "NULL")

```


```{r}
#take only response data
data_reduced <- data[,1:50]

#change to numeric
for(i in 1:50){
  data_reduced[,i]=as.numeric(data_reduced[,i])-1
}

pos_keyed_vars <-  c('EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',
                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', 
                    'EST8', 'EST9', 'EST10',
                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',
                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', 
                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 
                    'OPN10')
neg_keyed_vars <-  c('EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',
                    'EST2', 'EST4',
                    'AGR1', 'AGR3', 'AGR5', 'AGR7', 
                    'CSN2', 'CSN4', 'CSN6', 'CSN8', 
                    'OPN2', 'OPN4', 'OPN6')

for(key in neg_keyed_vars){
  data_reduced[key]=6-data_reduced[key]
}

score_data <- data_reduced %>% mutate(
                                    EST=rowSums(data_reduced[11:20])/10,
                                    EXT=rowSums(data_reduced[1:10])/10,
                                    OPN=rowSums(data_reduced[41:50])/10,
                                    AGR=rowSums(data_reduced[21:30])/10,
                                    CSN=rowSums(data_reduced[31:40])/10)
score_data_final <- score_data_final<-na.omit(score_data[,51:55])

```


```{r}
##############
#function to show k means result 
##############
kmeans_plot <- function(data,k ){
  centers <- kmeans(data, k)$centers
  par(mfrow=c(k/2,2))
  for(i in 1:k){
    plot(x=1:5,centers[i,],ylim = c(0,5),main=paste("means vector",i))
  }
}


```


```{r}
#plot
kmeans_plot(score_data_final,6)

```



```{r}
#normalcy check
for(i in 1:5){
  hist(score_data_final[,i])
}

```
```{r}
data.fa <- factanal(data_reduced, 5)
```





```{r}

data <- score_data_final[1:10000,]
data_scale <- scale(data)
dp <- DirichletProcessMvnormal(data_scale)
?DirichletProcessMvnormal
dp <- Fit(dp, 100)
dp$numberClusters
dp
dp$mixingDistribution
plot(dp)
dp$clusterParameters$mu[,,1]
par(mfrow=c(1,3))
for(i in 1:3){
 plot(x=1:5,dp$clusterParameters$mu[,,i],ylim = c(-3,3),main=paste("means vector",i)) 
}
dp$pointsPerCluster
dp$clusterLabels
dp$alpha
```






```{r}
my_data$labelsChain[[100]]
```


```{r}
my_data <- readRDS("my_data.rds")
my_data$clusterLabels
rgamma(1,2,4)
```

```{r}
data10000<-data_scale[1:10000,]
data10000 <- cbind(data10000, cluster = my_data$clusterLabels)
```







```{r}
library(bayestestR)
library("mvtnorm")



x3 <- rmvnorm(1000, my_data$clusterParameters$mu[,,3], my_data$clusterParameters$sig[,,3])

Overlap3<-c()
for(i in 1:my_data$numberClusters)
{ if (i==3 || i==7 || i==8||i==9)
  {Overlap3[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap3[i]=overlap(x3, y)}}
max(Overlap3)
which.max(Overlap3)

for(i in 1:10000)
{if(data10000[i,6]==3)
{data10000[i,6]<-1}
}


x7 <- rmvnorm(1000, my_data$clusterParameters$mu[,,7], my_data$clusterParameters$sig[,,7])

Overlap7<-c()
for(i in 1:my_data$numberClusters)
{ if (i==3 || i==7 || i==8||i==9)
  {Overlap7[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap7[i]=overlap(x7, y)}}
max(Overlap7)
which.max(Overlap7)

for(i in 1:10000)
{if(data10000[i,6]==7)
{data10000[i,6]<-5}
}


x8 <- rmvnorm(1000, my_data$clusterParameters$mu[,,7], my_data$clusterParameters$sig[,,7])

Overlap8<-c()
for(i in 1:my_data$numberClusters)
{ if (i==3 || i==7 || i==8||i==9)
  {Overlap8[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap8[i]=overlap(x8, y)}}
max(Overlap8)
which.max(Overlap8)

for(i in 1:10000)
{if(data10000[i,6]==8)
{data10000[i,6]<-5}
}


x9 <- rmvnorm(1000, my_data$clusterParameters$mu[,,7], my_data$clusterParameters$sig[,,7])

Overlap9<-c()
for(i in 1:my_data$numberClusters)
{ if (i==3 || i==7 || i==8||i==9)
  {Overlap9[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap9[i]=overlap(x9, y)}}
max(Overlap9)
which.max(Overlap9)

for(i in 1:10000)
{if(data10000[i,6]==9)
{data10000[i,6]<-5}
}

```

```{r}
library(Rfast)
df <- as.data.frame(data10000)
group1 <- df[df$cluster == 1, ]
group2 <- df[df$cluster == 2, ]
group3 <- df[df$cluster == 4, ]
group4 <- df[df$cluster == 5, ]
group5 <- df[df$cluster == 6, ]

matrix1 <- as.matrix(group1)
matrix1 <- matrix1[,-6]
matrix2 <- as.matrix(group2)
matrix2 <- matrix2[,-6]
matrix3 <- as.matrix(group3)
matrix3 <- matrix3[,-6]
matrix4 <- as.matrix(group4)
matrix4 <- matrix4[,-6]
matrix5 <- as.matrix(group5)
matrix5 <- matrix5[,-6]

r1=mvnorm.mle(matrix1)
r2=mvnorm.mle(matrix2)
r3=mvnorm.mle(matrix3)
r4=mvnorm.mle(matrix4)
r5=mvnorm.mle(matrix5)


p1=nrow(matrix1)/10000
p2=nrow(matrix2)/10000
p3=nrow(matrix3)/10000
p4=nrow(matrix4)/10000
p5=nrow(matrix5)/10000


```









```{r}
for(i in 1:9){
 plot(x=1:5,my_data$clusterParameters$mu[,,i],ylim = c(-3,3),main=paste("means vector",i)) 
}
```












## Test IGNORE
```{r}
food <- read.csv("https://userpage.fu-berlin.de/soga/300/30100_data_sets/food-texture.csv",
                 row.names = "X")
str(food)
food.fa <- factanal(food, factors = 2)
```

```{r}

library(mixAK)
Prior <- list(priorK = "fixed", Kmax = 10)
nMCMC <- c(burn=5000, keep=10000, thin=5, info=1000)
fit_personality <- NMixMCMC(y0=score_data_final[1:10000,],prior=Prior,nMCMC=nMCMC)

```
```{r}
fit_personality[[1]]$poster.mean.w
```

```{r}

## Simple analysis of Anderson's iris data
## ==============================================
library("colorspace")

data(iris, package="datasets")
summary(iris)
VARS <- names(iris)[1:4]
#COLS <- rainbow_hcl(3, start = 60, end = 240)
COLS <- c("red", "darkblue", "darkgreen")
names(COLS) <- levels(iris[, "Species"])

### Prior distribution and the length of MCMC
Prior <- list(priorK = "fixed", Kmax = 3)
nMCMC <- c(burn=5000, keep=10000, thin=5, info=1000)

### Run MCMC
set.seed(20091230)
fit <- NMixMCMC(y0 = iris[, VARS], prior = Prior, nMCMC = nMCMC)

### Basic posterior summary
print(fit)

### Univariate marginal posterior predictive densities
### based on chain #1
pdens1 <- NMixPredDensMarg(fit[[1]], lgrid=150)
plot(pdens1)
plot(pdens1, main=VARS, xlab=VARS)

### Bivariate (for each pair of margins) predictive densities
### based on chain #1
pdens2a <- NMixPredDensJoint2(fit[[1]])
plot(pdens2a)

plot(pdens2a, xylab=VARS)
plot(pdens2a, xylab=VARS, contour=TRUE)

### Determine the grid to compute bivariate densities
grid <- list(Sepal.Length=seq(3.5, 8.5, length=75),
             Sepal.Width=seq(1.8, 4.5, length=75),
             Petal.Length=seq(0, 7, length=75),
             Petal.Width=seq(-0.2, 3, length=75))
pdens2b <- NMixPredDensJoint2(fit[[1]], grid=grid)
plot(pdens2b, xylab=VARS)

### Plot with contours
ICOL <- rev(heat_hcl(20, c=c(80, 30), l=c(30, 90), power=c(1/5, 2)))
oldPar <- par(mfrow=c(2, 3), bty="n")
for (i in 1:3){
  for (j in (i+1):4){
    NAME <- paste(i, "-", j, sep="")
    MAIN <- paste(VARS[i], "x", VARS[j])
    image(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], col=ICOL,
          xlab=VARS[i], ylab=VARS[j], main=MAIN)
    contour(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], add=TRUE, col="brown4")
  }  
}  

### Plot with data
for (i in 1:3){
  for (j in (i+1):4){
    NAME <- paste(i, "-", j, sep="")
    MAIN <- paste(VARS[i], "x", VARS[j])
    image(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], col=ICOL,
          xlab=VARS[i], ylab=VARS[j], main=MAIN)
    for (spec in levels(iris[, "Species"])){
      Data <- subset(iris, Species==spec)
      points(Data[,i], Data[,j], pch=16, col=COLS[spec])
    }  
  }  
}  

### Set the graphical parameters back to their original values
par(oldPar)

### Clustering based on posterior summary statistics of component allocations
### or on the posterior distribution of component allocations
### (these are two equivalent estimators of probabilities of belonging
###  to each mixture components for each observation)
p1 <- fit[[1]]$poster.comp.prob_u
p2 <- fit[[1]]$poster.comp.prob_b

### Clustering based on posterior summary statistics of mixture weight, means, variances
p3 <- NMixPlugDA(fit[[1]], iris[, VARS])
p3 <- p3[, paste("prob", 1:3, sep="")]

  ### Observations from "setosa" species (all would be allocated in component 1)
apply(p1[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))

  ### Observations from "versicolor" species (almost all would be allocated in component 2)
apply(p1[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))

  ### Observations from "virginica" species (all would be allocated in component 3)
apply(p1[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))

## End(Not run)
```

