---
title: "EDA"
author: "Danny Luo"
date: "4/18/2021"
output: pdf_document
---
```{r}
library(dplyr)
library(dirichletprocess)
library(ggplot2)
```



```{r}
###model proposal

# x_i | z_i=j ~  multivariate gaussian mixture (mu_j,sigma_j)   i=1,..,n
# mu_j,sigma_j ~ Normal-InvGamma for j = 1,...,k    
# z_i ~ categorical (k category, prob vector p)  for i=1,..,n
# p ~ Dirichlet(\alpha)  

#Interested in:  joint_posterior (p,z, {mu_j,sig_j}^k )  using Gibbs Sampler

#packages 



#x_i would be of dimension 5 (originally 50)
#Gaussian assumption?
#more sophisticatd models different Bayesian clustering
#choices of distance functions. sum of square



#z vector of size 1 million
#At each iteration, we need to sample z_i for i=1,...,n
#thin data due to computation cost?


#choose value k? 


#k-mean as prior?




```

```{r}
data_raw = read.csv("data-final.csv", sep='\t', na.strings = "NULL")
```

```{r}
data_raw
```



```{r}
data = data.frame(data_raw)

data <- data[ -c(51:107) ]
data <- data[ -c(52:53) ]

print(nrow(data))
head(data)
data <- na.omit(data)
nrow(data)
```

```{r}
data <- na.omit(data)
nrow(data)
```
```{r}
pos_keyed_vars <-  c('EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',
                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', 
                    'EST8', 'EST9', 'EST10',
                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',
                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', 
                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 
                    'OPN10')
neg_keyed_vars <-  c('EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',
                    'EST2', 'EST4',
                    'AGR1', 'AGR3', 'AGR5', 'AGR7', 
                    'CSN2', 'CSN4', 'CSN6', 'CSN8', 
                    'OPN2', 'OPN4', 'OPN6')
```

```{r}
for(key in neg_keyed_vars){
  data[key]=6-data[key]
}
```

```{r}
data <- data %>% mutate(
  EXT=rowSums(data[1:10]),
  EST=rowSums(data[11:20]),
  AGR=rowSums(data[21:30]),
  CSN=rowSums(data[31:40]),
  OPN=rowSums(data[41:50]))
score_data_final <- data[,52:56]
```

```{r, message = FALSE}
traits = c('EXT', 'EST', 'AGR', 'CSN', 'OPN')
trait_labels = c('Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness')
myplots <- list()
for(i in 1:5){
  p1 <- ggplot(score_data_final, aes(x= score_data_final[,i] ))+
    geom_histogram(colour="black", fill="lightblue",binwidth=3)+
    theme_minimal()+
    theme(axis.title.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank())+
    labs(title= trait_labels[i])
  print(p1)
}
```


```{r}
score_data_final <- scale(score_data_final)
score_data_final<-as.data.frame(score_data_final)
score_data_final
```

```{r}
data_sample = sample_n(score_data_final,10000)
data_sample = as.matrix(data)
head(data_sample)
```


#DO NOT TOUCH

```{r}

dp <- DirichletProcessMvnormal(data)
?DirichletProcessMvnormal
dp <- Fit(dp, 1000)
saveRDS(dp, file = "DP_result.rds")
```









```{r}
dp$numberClusters
dp
dp$mixingDistribution
plot(dp)
dp$clusterParameters$mu[,,1]
par(mfrow=c(1,3))
for(i in 1:12){
 plot(x=1:5,dp$clusterParameters$mu[,,i],ylim = c(-3,3),main=paste("means vector",i)) 
}
dp$pointsPerCluster
dp$clusterLabels
dp$alpha
```






```{r}
my_data$labelsChain[[100]]
```



```{r}

my_data <- readRDS("DP_result.rds")
my_data$numberClusters
my_data$pointsPerCluster
my_data$weights


my_data <- readRDS("my_data.rds")
my_data$clusterLabels
rgamma(1,2,4)

```

```{r}
data10000<-data[1:10000,]
data10000 <- cbind(data10000, cluster = my_data$clusterLabels)
```

```{r}
Overlap<-matrix( 0,nrow = 9, ncol = 2)

for(j in 1:my_data$numberClusters)
{x <- rmvnorm(1000, my_data$clusterParameters$mu[,,j], my_data$clusterParameters$sig[,,j])
temp<-c()

for(i in 1:my_data$numberClusters)
{ if (i==j)
  {temp[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

temp[i]=overlap(x, y)}}
Overlap[j,1]<-max(temp)
Overlap[j,2]<-which.max(temp)
}


Overlap
```


Check overlap with other groups
```{r}
library("bayestestR")
library("mvtnorm")

x10 <- rmvnorm(1000, my_data$clusterParameters$mu[,,10], my_data$clusterParameters$sig[,,10])

Overlap10<-c()
for(i in 1:my_data$numberClusters)
{ if ( i==10 || i==11||i==12 )
  {Overlap7[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap10[i]=overlap(x10, y)}}
max(Overlap10)
which.max(Overlap10)



x11 <- rmvnorm(1000, my_data$clusterParameters$mu[,,11], my_data$clusterParameters$sig[,,11])

Overlap11<-c()
for(i in 1:my_data$numberClusters)
{ if ( i==10 || i==11||i==12)
  {Overlap11[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap11[i]=overlap(x11, y)}}
max(Overlap11)
which.max(Overlap11)


x12 <- rmvnorm(1000, my_data$clusterParameters$mu[,,12], my_data$clusterParameters$sig[,,12])

Overlap12<-c()
for(i in 1:my_data$numberClusters)
{ if ( i==10 || i==11||i==12)
  {Overlap12[i]= 0
    next}
  
  else
 { y <- rmvnorm(1000, my_data$clusterParameters$mu[,,i], my_data$clusterParameters$sig[,,i])

Overlap12[i]=overlap(x12, y)}}
max(Overlap12)
which.max(Overlap12)


```

<<<<<<< HEAD


#Consider if we need to refit the normal model here---does it worth to recalculate the points for just 3 or 2 or 1 point?
=======
>>>>>>> b1c3515ecbcf56ef95fdb6ecd84026ef5aad2387
```{r}
library(Rfast)
df <- as.data.frame(data10000)
group1 <- df[df$cluster == 1, ]
group2 <- df[df$cluster == 2, ]
group3 <- df[df$cluster == 4, ]
group4 <- df[df$cluster == 5, ]
group5 <- df[df$cluster == 6, ]

matrix1 <- as.matrix(group1)
matrix1 <- matrix1[,-6]
matrix2 <- as.matrix(group2)
matrix2 <- matrix2[,-6]
matrix3 <- as.matrix(group3)
matrix3 <- matrix3[,-6]
matrix4 <- as.matrix(group4)
matrix4 <- matrix4[,-6]
matrix5 <- as.matrix(group5)
matrix5 <- matrix5[,-6]

r1=mvnorm.mle(matrix1)
r2=mvnorm.mle(matrix2)
r3=mvnorm.mle(matrix3)
r4=mvnorm.mle(matrix4)
r5=mvnorm.mle(matrix5)


p1=nrow(matrix1)/10000
p2=nrow(matrix2)/10000
p3=nrow(matrix3)/10000
p4=nrow(matrix4)/10000
p5=nrow(matrix5)/10000



```









```{r}

 plot(x=1:5,r1$mu,ylim = c(-3,3),main=paste("means vector",1)) 
 plot(x=1:5,r2$mu,ylim = c(-3,3),main=paste("means vector",2)) 
  plot(x=1:5,r3$mu,ylim = c(-3,3),main=paste("means vector",3)) 
   plot(x=1:5,r4$mu,ylim = c(-3,3),main=paste("means vector",4)) 
    plot(x=1:5,r5$mu,ylim = c(-3,3),main=paste("means vector",5)) 
    
```





<<<<<<< HEAD
=======







## Test IGNORE
```{r}
food <- read.csv("https://userpage.fu-berlin.de/soga/300/30100_data_sets/food-texture.csv",
                 row.names = "X")
str(food)
food.fa <- factanal(food, factors = 2)
```
>>>>>>> b1c3515ecbcf56ef95fdb6ecd84026ef5aad2387

```{r}

library(mixAK)
Prior <- list(priorK = "fixed", Kmax = 9)
Mu=rbind(c(my_data$clusterParameters$mu[,,1]),c(my_data$clusterParameters$mu[,,2]),c(my_data$clusterParameters$mu[,,3]),c(my_data$clusterParameters$mu[,,4]),c(my_data$clusterParameters$mu[,,5]),my_data$clusterParameters$mu[,,6],my_data$clusterParameters$mu[,,7],my_data$clusterParameters$mu[,,8],my_data$clusterParameters$mu[,,9])
Sig=mat_combined1 <- rbind(my_data$clusterParameters$sig[,,1], my_data$clusterParameters$sig[,,2 ],my_data$clusterParameters$sig[,,3 ],my_data$clusterParameters$sig[,,4 ],my_data$clusterParameters$sig[,,5 ],my_data$clusterParameters$sig[,,6 ],my_data$clusterParameters$sig[,,7 ],my_data$clusterParameters$sig[,,8 ],my_data$clusterParameters$sig[,,9 ]) 


Init<-list(K=9, w=c(my_data$weights[1],my_data$weights[2],my_data$weights[3],my_data$weights[4],my_data$weights[5],my_data$weights[6],my_data$weights[7],my_data$weights[8],my_data$weights[9]),mu=Mu, Sigma=Sig)


Matrix1=
NMCMC <- c(burn=1000, keep=5000, thin=5, info=1000)

fit_personality <- NMixMCMC(y0=score_data_final[0:20000,],prior=Prior,init=Init,nMCMC=NMCMC)

```
```{r}

fit_personality[[1]]$poster.mean.w
fit_personality[[1]]$poster.mean.mu
fit_personality[[1]]$poster.mean.Sigma
```


```{r}
plot(fit_personality[[1]]$poster.mean.mu[2,],ylim = c(-2,2))
plot(fit_personality[[1]]$poster.mean.mu[3,],ylim = c(-2,2))
plot(fit_personality[[1]]$poster.mean.mu[4,],ylim = c(-2,2))
plot(fit_personality[[1]]$poster.mean.mu[5,],ylim = c(-2,2))
```


```{r}

## Simple analysis of Anderson's iris data
## ==============================================
library("colorspace")

data(iris, package="datasets")
summary(iris)
VARS <- names(iris)[1:4]
#COLS <- rainbow_hcl(3, start = 60, end = 240)
COLS <- c("red", "darkblue", "darkgreen")
names(COLS) <- levels(iris[, "Species"])

### Prior distribution and the length of MCMC
Prior <- list(priorK = "fixed", Kmax = 3)
nMCMC <- c(burn=5000, keep=10000, thin=5, info=1000)

### Run MCMC
set.seed(20091230)
fit <- NMixMCMC(y0 = iris[, VARS], prior = Prior, nMCMC = nMCMC)

### Basic posterior summary
print(fit)

### Univariate marginal posterior predictive densities
### based on chain #1
pdens1 <- NMixPredDensMarg(fit[[1]], lgrid=150)
plot(pdens1)
plot(pdens1, main=VARS, xlab=VARS)

### Bivariate (for each pair of margins) predictive densities
### based on chain #1
pdens2a <- NMixPredDensJoint2(fit[[1]])
plot(pdens2a)

plot(pdens2a, xylab=VARS)
plot(pdens2a, xylab=VARS, contour=TRUE)

### Determine the grid to compute bivariate densities
grid <- list(Sepal.Length=seq(3.5, 8.5, length=75),
             Sepal.Width=seq(1.8, 4.5, length=75),
             Petal.Length=seq(0, 7, length=75),
             Petal.Width=seq(-0.2, 3, length=75))
pdens2b <- NMixPredDensJoint2(fit[[1]], grid=grid)
plot(pdens2b, xylab=VARS)

### Plot with contours
ICOL <- rev(heat_hcl(20, c=c(80, 30), l=c(30, 90), power=c(1/5, 2)))
oldPar <- par(mfrow=c(2, 3), bty="n")
for (i in 1:3){
  for (j in (i+1):4){
    NAME <- paste(i, "-", j, sep="")
    MAIN <- paste(VARS[i], "x", VARS[j])
    image(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], col=ICOL,
          xlab=VARS[i], ylab=VARS[j], main=MAIN)
    contour(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], add=TRUE, col="brown4")
  }  
}  

### Plot with data
for (i in 1:3){
  for (j in (i+1):4){
    NAME <- paste(i, "-", j, sep="")
    MAIN <- paste(VARS[i], "x", VARS[j])
    image(pdens2b$x[[i]], pdens2b$x[[j]], pdens2b$dens[[NAME]], col=ICOL,
          xlab=VARS[i], ylab=VARS[j], main=MAIN)
    for (spec in levels(iris[, "Species"])){
      Data <- subset(iris, Species==spec)
      points(Data[,i], Data[,j], pch=16, col=COLS[spec])
    }  
  }  
}  

### Set the graphical parameters back to their original values
par(oldPar)

### Clustering based on posterior summary statistics of component allocations
### or on the posterior distribution of component allocations
### (these are two equivalent estimators of probabilities of belonging
###  to each mixture components for each observation)
p1 <- fit[[1]]$poster.comp.prob_u
p2 <- fit[[1]]$poster.comp.prob_b

### Clustering based on posterior summary statistics of mixture weight, means, variances
p3 <- NMixPlugDA(fit[[1]], iris[, VARS])
p3 <- p3[, paste("prob", 1:3, sep="")]

  ### Observations from "setosa" species (all would be allocated in component 1)
apply(p1[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[1:50,], 2, quantile, prob=seq(0, 1, by=0.1))

  ### Observations from "versicolor" species (almost all would be allocated in component 2)
apply(p1[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[51:100,], 2, quantile, prob=seq(0, 1, by=0.1))

  ### Observations from "virginica" species (all would be allocated in component 3)
apply(p1[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p2[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))
apply(p3[101:150,], 2, quantile, prob=seq(0, 1, by=0.1))

## End(Not run)
```

